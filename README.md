# Fine tune Flan-T5 model with Low-Rank Adaptation on dialog summarization task
## Objective
The goal of this notebook is to provide a template for training LLMs using state-of-the-art techniques. We go through the process of fine tuning Google's Flan-T5 large language model on dialog summarization task using Low-Rank Adaptation. We also perform hyperparameter tuning with Ray Tune library. Our experiments are tracked by Weights & Biases.
